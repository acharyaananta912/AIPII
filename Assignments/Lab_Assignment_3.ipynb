{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayem1997/AIPII/blob/master/Assignments/Lab_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePDOnMfL7HWV",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to AI Programming II:: Assignment #3 (Lab)  -- Total: 50 pts(Each points are 10 pts.)\n",
        "\n",
        "### Write your information in below. \n",
        "### Student ID:                        \n",
        "### Name: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-lp_zSu309l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow & keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P-iGK0537fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download mnist data from MNIST server\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HU8nFIi4Ew3",
        "colab_type": "code",
        "outputId": "5013efa9-1d5f-4c17-af05-3df4b4a49d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# Visualize images of the MNIST dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_train[i]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX_Nnd5-4G8t",
        "colab_type": "code",
        "outputId": "d2fb7571-442d-4e32-aa32-c2d6f8038a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check the shape of training data\n",
        "print(x_train.shape)\n",
        "# check the shape of test data\n",
        "print(x_test.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zvLieso4wP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten training and test datasets to 1-d\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNi9ALPp5gpV",
        "colab_type": "code",
        "outputId": "c77e8534-02cb-4f84-cc8b-c29afdfc33d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train[0].shape, x_test[0].shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((784,), (784,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7ONnDu5im0",
        "colab_type": "code",
        "outputId": "b645ac2d-fcdf-4961-f50f-a77b374dd437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# observe the shapes of training data after the reshape\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 784)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlYf7Gg5mFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1,2,3,... 10 :One Hot coding\n",
        "num_classes = 10\n",
        "from keras.utils import np_utils\n",
        "# reshape labels categorically\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqPUM2Lr5pwW",
        "colab_type": "code",
        "outputId": "25c7d69c-37a0-4279-dc13-3beed0bb1996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(y_train[0])\n",
        "print(y_test[0])\n",
        "y_train.shape,y_test.shape\n",
        "print(x_train.shape[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnpGPKF45sNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(num_layers):\n",
        "    model= Sequential()\n",
        "    if num_layers == 0:\n",
        "        model.add(Dense(output_dim= num_classes, input_dim = x_train.shape[1], activation='softmax'))\n",
        "        model.compile(loss='mse', optimizer = 'sgd', metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "        if i == 0:\n",
        "            model.add(Dense(output_dim = 32, input_dim = x_train.shape[1], activation='sigmoid'))\n",
        "        else:\n",
        "            model.add(Dense(output_dim = 10, activation='sigmoid'))\n",
        "    model.add(Dense(output_dim = num_classes, activation=\"softmax\"))\n",
        "    model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7BpXJOo5-m8",
        "colab_type": "code",
        "outputId": "c77f150f-95a5-41fe-bfa2-081d1946865d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "(x_train.shape[1],)\n",
        "#num_hidden_neurons = 32\n",
        "model = make_model(num_layers=1)\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(x_train, y_train, batch_size = 128, epochs = 5, validation_data  =(x_test, y_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_87 (Dense)             (None, 32)                25120     \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 25,450\n",
            "Trainable params: 25,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"sigmoid\", units=32)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0918 - accuracy: 0.1159 - val_loss: 0.0913 - val_accuracy: 0.1255\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0911 - accuracy: 0.1367 - val_loss: 0.0907 - val_accuracy: 0.1548\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0905 - accuracy: 0.1642 - val_loss: 0.0902 - val_accuracy: 0.1772\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0901 - accuracy: 0.1849 - val_loss: 0.0898 - val_accuracy: 0.1950\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0897 - accuracy: 0.1986 - val_loss: 0.0894 - val_accuracy: 0.2075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWarTulI6DN0",
        "colab_type": "code",
        "outputId": "368c7f17-dd60-43db-b52c-4dcf0ae7b378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#evaluate the test loss\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.08941421670913696\n",
            "Test accuracy: 0.20749999582767487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LfYCahv6uxu",
        "colab_type": "code",
        "outputId": "9ac77f75-ebbf-4e48-b64f-b3ed3396e207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# test the model through visually debugging the inputs,,  run > twice to see the image\n",
        "# index of your input to test\n",
        "n = 3\n",
        "# see the image by plotting\n",
        "plt.imshow(x_test[n].reshape(28, 28), cmap='Greys', interpolation='nearest')  \n",
        "plt.show()\n",
        "\n",
        "# your model's prediction, is the output correct?\n",
        "print('The Answer is ', model.predict_classes(x_test[n].reshape((1, 28* 28) ) ) )\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The Answer is  [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLVXu5zs7tji",
        "colab_type": "text"
      },
      "source": [
        "## 1. Report the result of the test loss & accuracy, when we set the dimension(neurons) of hidden layer to (64, 128, 256) with one hidden layer.\n",
        "## *** Default condition : refer in the above codes.. \n",
        "## *** You have to write your codes in another blank area, and write your answer in below.\n",
        "\n",
        "> 들여쓴 블록\n",
        "\n",
        "\n",
        "| dimension |      test loss     |    test accuracy      |\n",
        "|-----------|--------------------|-----------------------|\n",
        "|   64      |                    |                       |\n",
        "|-----------|--------------------|-----------------------|\n",
        "|   128     |                    |                       | \n",
        "|-----------|--------------------|-----------------------|\n",
        "|   256     |                    |                       |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YMxRSVW7_HC",
        "colab_type": "code",
        "outputId": "0cfdef58-654b-4638-a5fa-86907807e0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your answer in here\n",
        "def make_model(num_layers, neurons, act='sigmoid'):\n",
        "    model= Sequential()\n",
        "    if num_layers == 0:\n",
        "        model.add(Dense(output_dim= num_classes, input_dim = x_train.shape[1], activation='softmax'))\n",
        "        model.compile(loss='mse', optimizer = 'sgd', metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "            model.add(Dense(output_dim = neurons, input_dim = x_train.shape[1], activation=act))\n",
        "    \n",
        "    model.add(Dense(output_dim = num_classes, activation=\"softmax\"))\n",
        "    \n",
        "    model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "dimensions = [64, 128, 256]\n",
        "for dimension in dimensions:\n",
        "  model = make_model(1, dimension, 'sigmoid')\n",
        "  model.summary()\n",
        "  history = model.fit(x_train, y_train, batch_size = 128, epochs = 5, validation_data  =(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Accuracy for ', dimension, ' number of neorons:')\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1], '\\n\\n')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_89 (Dense)             (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"sigmoid\", units=64)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0916 - accuracy: 0.1027 - val_loss: 0.0910 - val_accuracy: 0.1040\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0907 - accuracy: 0.1075 - val_loss: 0.0903 - val_accuracy: 0.1168\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0901 - accuracy: 0.1257 - val_loss: 0.0898 - val_accuracy: 0.1408\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0896 - accuracy: 0.1621 - val_loss: 0.0893 - val_accuracy: 0.1923\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0891 - accuracy: 0.2299 - val_loss: 0.0889 - val_accuracy: 0.2621\n",
            "Accuracy for  64  number of neorons:\n",
            "Test loss: 0.08889823598861694\n",
            "Test accuracy: 0.2621000111103058 \n",
            "\n",
            "\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_91 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"sigmoid\", units=128)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0918 - accuracy: 0.1092 - val_loss: 0.0911 - val_accuracy: 0.1333\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0907 - accuracy: 0.1490 - val_loss: 0.0903 - val_accuracy: 0.1706\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0900 - accuracy: 0.1806 - val_loss: 0.0897 - val_accuracy: 0.1976\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0895 - accuracy: 0.2059 - val_loss: 0.0892 - val_accuracy: 0.2227\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0891 - accuracy: 0.2286 - val_loss: 0.0888 - val_accuracy: 0.2464\n",
            "Accuracy for  128  number of neorons:\n",
            "Test loss: 0.08879808113574982\n",
            "Test accuracy: 0.24639999866485596 \n",
            "\n",
            "\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_93 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 203,530\n",
            "Trainable params: 203,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"sigmoid\", units=256)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0920 - accuracy: 0.1079 - val_loss: 0.0911 - val_accuracy: 0.1140\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0906 - accuracy: 0.1258 - val_loss: 0.0903 - val_accuracy: 0.1434\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0900 - accuracy: 0.1564 - val_loss: 0.0897 - val_accuracy: 0.1719\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0895 - accuracy: 0.1935 - val_loss: 0.0893 - val_accuracy: 0.2213\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0891 - accuracy: 0.2395 - val_loss: 0.0889 - val_accuracy: 0.2591\n",
            "Accuracy for  256  number of neorons:\n",
            "Test loss: 0.08890838930606842\n",
            "Test accuracy: 0.2590999901294708 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMEPBy09wu8",
        "colab_type": "text"
      },
      "source": [
        "## 2. Report the result of the test loss & accuracy, when we set the number of hidden layer to (2, 4, 8)  \n",
        "## *** Default condition : refer in the above codes and each hidden layer have same neurons(number of neurons: 128) \n",
        "## *** You have to write your codes in another blank area, and write your answer in below.\n",
        "\n",
        "> 들여쓴 블록\n",
        "\n",
        "\n",
        "| layer's num |      test loss     |    test accuracy      |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   2         |                    |                       |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   4         |                    |                       | \n",
        "|-------------|--------------------|-----------------------|\n",
        "|   8         |                    |                       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SluhPBjM9P3j",
        "colab_type": "code",
        "outputId": "076b73a3-c191-4d8f-cae1-efc5062d2e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your answer in here \n",
        "layers = [2, 4, 8]\n",
        "for layer in layers:\n",
        "  model = make_model(layer, 128, 'sigmoid')\n",
        "  model.summary()\n",
        "  history = model.fit(x_train, y_train, batch_size = 128, epochs = 5, validation_data  =(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Accuracy for ', layer, ' number of layers:')\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1], '\\n\\n')\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_95 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"sigmoid\", units=128)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0917 - accuracy: 0.1017 - val_loss: 0.0912 - val_accuracy: 0.0975\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0909 - accuracy: 0.0899 - val_loss: 0.0907 - val_accuracy: 0.0811\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0905 - accuracy: 0.0915 - val_loss: 0.0905 - val_accuracy: 0.0930\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0903 - accuracy: 0.1037 - val_loss: 0.0903 - val_accuracy: 0.1009\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0902 - accuracy: 0.1124 - val_loss: 0.0901 - val_accuracy: 0.1241\n",
            "Accuracy for  2  number of layers:\n",
            "Test loss: 0.09013829247951508\n",
            "Test accuracy: 0.12409999966621399 \n",
            "\n",
            "\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_98 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 151,306\n",
            "Trainable params: 151,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.0926 - accuracy: 0.1124 - val_loss: 0.0917 - val_accuracy: 0.1135\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0912 - accuracy: 0.1124 - val_loss: 0.0909 - val_accuracy: 0.1135\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0906 - accuracy: 0.1124 - val_loss: 0.0905 - val_accuracy: 0.1135\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0904 - accuracy: 0.1124 - val_loss: 0.0903 - val_accuracy: 0.1135\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0902 - accuracy: 0.1124 - val_loss: 0.0902 - val_accuracy: 0.1135\n",
            "Accuracy for  4  number of layers:\n",
            "Test loss: 0.09016003012657166\n",
            "Test accuracy: 0.11349999904632568 \n",
            "\n",
            "\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_103 (Dense)            (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 217,354\n",
            "Trainable params: 217,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 0.0921 - accuracy: 0.1022 - val_loss: 0.0914 - val_accuracy: 0.1010\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0911 - accuracy: 0.1022 - val_loss: 0.0908 - val_accuracy: 0.1010\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0907 - accuracy: 0.1022 - val_loss: 0.0906 - val_accuracy: 0.1010\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 41us/step - loss: 0.0905 - accuracy: 0.1022 - val_loss: 0.0904 - val_accuracy: 0.1010\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 42us/step - loss: 0.0904 - accuracy: 0.1094 - val_loss: 0.0903 - val_accuracy: 0.1135\n",
            "Accuracy for  8  number of layers:\n",
            "Test loss: 0.09027811253070832\n",
            "Test accuracy: 0.11349999904632568 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrZcRg85_O3j",
        "colab_type": "text"
      },
      "source": [
        "## 3. Report the result of the test loss & accuracy, when we set the number of batch size to (64, 128, 256) \n",
        "## *** Default condition : refer in the above codes,  use \"relu\" activation, dropout rate(0.2) and each hidden layer have same neurons(number of neurons: 256) \n",
        "## *** You have to write your codes in another blank area, and write your answer in below.\n",
        "\n",
        "> 들여쓴 블록\n",
        "\n",
        "\n",
        "| batch size  |      test loss     |    test accuracy      |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   64        |                    |                       |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   128       |                    |                       | \n",
        "|-------------|--------------------|-----------------------|\n",
        "|   256       |                    |                       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-B7cNa7_Dp6",
        "colab_type": "code",
        "outputId": "f7c05a76-8200-4e77-87f1-d39d16c90e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your answer in here\n",
        "def make_model(num_layers, neurons, act='sigmoid'):\n",
        "    model= Sequential()\n",
        "    if num_layers == 0:\n",
        "        model.add(Dense(output_dim= num_classes, input_dim = x_train.shape[1], activation='softmax'))\n",
        "        model.compile(loss='mse', optimizer = 'sgd', metrics=['accuracy'])\n",
        "        return model\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "            model.add(Dense(output_dim = neurons, input_dim = x_train.shape[1], activation=act))\n",
        "            model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(output_dim = num_classes, activation=\"softmax\"))\n",
        "    \n",
        "    model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = make_model(8, 256, 'relu')\n",
        "model.summary()\n",
        "\n",
        "batch_sizes = [64, 128, 256]\n",
        "for batch_size in batch_sizes:\n",
        "  history = model.fit(x_train, y_train, batch_size = batch_size, epochs = 5, validation_data  =(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Accuracy for ', batch_size, ' Batch Size:')\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1], '\\n\\n')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"relu\", units=256)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_112 (Dense)            (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 664,074\n",
            "Trainable params: 664,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0901 - accuracy: 0.1096 - val_loss: 0.0900 - val_accuracy: 0.1135\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0901 - accuracy: 0.1096 - val_loss: 0.0900 - val_accuracy: 0.1122\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0901 - accuracy: 0.1089 - val_loss: 0.0900 - val_accuracy: 0.1089\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0900 - accuracy: 0.1092 - val_loss: 0.0900 - val_accuracy: 0.1067\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1088 - val_loss: 0.0899 - val_accuracy: 0.1054\n",
            "Accuracy for  64  Batch Size:\n",
            "Test loss: 0.08994089195728303\n",
            "Test accuracy: 0.10540000349283218 \n",
            "\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0900 - accuracy: 0.1099 - val_loss: 0.0899 - val_accuracy: 0.1070\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0900 - accuracy: 0.1081 - val_loss: 0.0899 - val_accuracy: 0.1099\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0900 - accuracy: 0.1103 - val_loss: 0.0899 - val_accuracy: 0.1124\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0900 - accuracy: 0.1094 - val_loss: 0.0899 - val_accuracy: 0.1164\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0900 - accuracy: 0.1092 - val_loss: 0.0899 - val_accuracy: 0.1248\n",
            "Accuracy for  128  Batch Size:\n",
            "Test loss: 0.08990029262304305\n",
            "Test accuracy: 0.12479999661445618 \n",
            "\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0900 - accuracy: 0.1117 - val_loss: 0.0899 - val_accuracy: 0.1283\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0900 - accuracy: 0.1119 - val_loss: 0.0899 - val_accuracy: 0.1329\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0900 - accuracy: 0.1118 - val_loss: 0.0899 - val_accuracy: 0.1374\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0900 - accuracy: 0.1112 - val_loss: 0.0899 - val_accuracy: 0.1410\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0900 - accuracy: 0.1103 - val_loss: 0.0899 - val_accuracy: 0.1455\n",
            "Accuracy for  256  Batch Size:\n",
            "Test loss: 0.08988013992309571\n",
            "Test accuracy: 0.14550000429153442 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXbCwzD_ANrV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Report the result of the test loss & accuracy, when we set the number of epochs to (100, 200, 300)\n",
        "*** Default condition :same with the above problem 3\n",
        "\n",
        "*** You have to write your codes in another blank area, and write your answer in below.\n",
        "\n",
        "\n",
        "| epochs      |      test loss     |    test accuracy      |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   100       |                    |                       |\n",
        "|-------------|--------------------|-----------------------|\n",
        "|   200       |                    |                       | \n",
        "|-------------|--------------------|-----------------------|\n",
        "|   300       |                    |                       |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tslahpgAhZy",
        "colab_type": "code",
        "outputId": "750b1f26-3ff1-49ca-a938-cb908488c77f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your answer in here\n",
        "\n",
        "model = make_model(8, 256, 'relu')\n",
        "history = model.fit(x_train, y_train, batch_size = 64, epochs = 100, validation_data  =(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy for ', 100, ' epochs:')\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1], '\\n\\n')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"relu\", units=256)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0901 - accuracy: 0.1000 - val_loss: 0.0900 - val_accuracy: 0.0831\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0901 - accuracy: 0.0998 - val_loss: 0.0900 - val_accuracy: 0.0910\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1004 - val_loss: 0.0900 - val_accuracy: 0.1283\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0900 - accuracy: 0.1030 - val_loss: 0.0900 - val_accuracy: 0.1547\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1068 - val_loss: 0.0899 - val_accuracy: 0.1654\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1067 - val_loss: 0.0899 - val_accuracy: 0.1694\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1112 - val_loss: 0.0899 - val_accuracy: 0.1722\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0900 - accuracy: 0.1161 - val_loss: 0.0899 - val_accuracy: 0.1721\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0900 - accuracy: 0.1208 - val_loss: 0.0899 - val_accuracy: 0.1736\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1230 - val_loss: 0.0899 - val_accuracy: 0.1725\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1274 - val_loss: 0.0898 - val_accuracy: 0.1712\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1334 - val_loss: 0.0898 - val_accuracy: 0.1736\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1328 - val_loss: 0.0898 - val_accuracy: 0.1744\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1334 - val_loss: 0.0898 - val_accuracy: 0.1762\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1364 - val_loss: 0.0898 - val_accuracy: 0.1784\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1428 - val_loss: 0.0898 - val_accuracy: 0.1839\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1432 - val_loss: 0.0897 - val_accuracy: 0.1886\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1484 - val_loss: 0.0897 - val_accuracy: 0.1918\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0898 - accuracy: 0.1500 - val_loss: 0.0897 - val_accuracy: 0.1956\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1530 - val_loss: 0.0897 - val_accuracy: 0.2004\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1577 - val_loss: 0.0896 - val_accuracy: 0.2067\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1594 - val_loss: 0.0896 - val_accuracy: 0.2138\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1633 - val_loss: 0.0896 - val_accuracy: 0.2207\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1652 - val_loss: 0.0895 - val_accuracy: 0.2269\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0897 - accuracy: 0.1686 - val_loss: 0.0895 - val_accuracy: 0.2338\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1721 - val_loss: 0.0895 - val_accuracy: 0.2413\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1716 - val_loss: 0.0894 - val_accuracy: 0.2470\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0896 - accuracy: 0.1773 - val_loss: 0.0894 - val_accuracy: 0.2511\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0896 - accuracy: 0.1785 - val_loss: 0.0893 - val_accuracy: 0.2570\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0896 - accuracy: 0.1831 - val_loss: 0.0893 - val_accuracy: 0.2615\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0895 - accuracy: 0.1874 - val_loss: 0.0892 - val_accuracy: 0.2659\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0895 - accuracy: 0.1871 - val_loss: 0.0892 - val_accuracy: 0.2694\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0895 - accuracy: 0.1906 - val_loss: 0.0891 - val_accuracy: 0.2724\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0894 - accuracy: 0.1961 - val_loss: 0.0890 - val_accuracy: 0.2724\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0893 - accuracy: 0.1971 - val_loss: 0.0889 - val_accuracy: 0.2714\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0893 - accuracy: 0.2004 - val_loss: 0.0888 - val_accuracy: 0.2694\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0892 - accuracy: 0.2055 - val_loss: 0.0887 - val_accuracy: 0.2638\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0892 - accuracy: 0.2048 - val_loss: 0.0885 - val_accuracy: 0.2582\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0890 - accuracy: 0.2069 - val_loss: 0.0884 - val_accuracy: 0.2541\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0889 - accuracy: 0.2116 - val_loss: 0.0882 - val_accuracy: 0.2482\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0888 - accuracy: 0.2144 - val_loss: 0.0880 - val_accuracy: 0.2392\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0886 - accuracy: 0.2191 - val_loss: 0.0877 - val_accuracy: 0.2347\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0884 - accuracy: 0.2202 - val_loss: 0.0874 - val_accuracy: 0.2288\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0882 - accuracy: 0.2229 - val_loss: 0.0870 - val_accuracy: 0.2240\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0879 - accuracy: 0.2244 - val_loss: 0.0865 - val_accuracy: 0.2194\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0875 - accuracy: 0.2252 - val_loss: 0.0859 - val_accuracy: 0.2166\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0870 - accuracy: 0.2268 - val_loss: 0.0851 - val_accuracy: 0.2162\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0864 - accuracy: 0.2294 - val_loss: 0.0842 - val_accuracy: 0.2177\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0857 - accuracy: 0.2324 - val_loss: 0.0832 - val_accuracy: 0.2225\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0849 - accuracy: 0.2358 - val_loss: 0.0823 - val_accuracy: 0.2346\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0841 - accuracy: 0.2438 - val_loss: 0.0814 - val_accuracy: 0.2460\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0833 - accuracy: 0.2513 - val_loss: 0.0806 - val_accuracy: 0.2557\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0826 - accuracy: 0.2630 - val_loss: 0.0797 - val_accuracy: 0.2656\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0818 - accuracy: 0.2736 - val_loss: 0.0788 - val_accuracy: 0.2721\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0810 - accuracy: 0.2853 - val_loss: 0.0777 - val_accuracy: 0.2804\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0802 - accuracy: 0.2979 - val_loss: 0.0766 - val_accuracy: 0.2881\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0793 - accuracy: 0.3104 - val_loss: 0.0752 - val_accuracy: 0.2975\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0782 - accuracy: 0.3235 - val_loss: 0.0736 - val_accuracy: 0.3152\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0771 - accuracy: 0.3411 - val_loss: 0.0718 - val_accuracy: 0.3377\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0756 - accuracy: 0.3557 - val_loss: 0.0696 - val_accuracy: 0.3710\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0740 - accuracy: 0.3742 - val_loss: 0.0671 - val_accuracy: 0.4023\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0722 - accuracy: 0.3897 - val_loss: 0.0644 - val_accuracy: 0.4406\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0704 - accuracy: 0.4040 - val_loss: 0.0621 - val_accuracy: 0.4733\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0687 - accuracy: 0.4178 - val_loss: 0.0601 - val_accuracy: 0.5011\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0672 - accuracy: 0.4339 - val_loss: 0.0585 - val_accuracy: 0.5167\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0658 - accuracy: 0.4439 - val_loss: 0.0570 - val_accuracy: 0.5350\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0645 - accuracy: 0.4602 - val_loss: 0.0556 - val_accuracy: 0.5488\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0632 - accuracy: 0.4769 - val_loss: 0.0540 - val_accuracy: 0.5761\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0619 - accuracy: 0.4916 - val_loss: 0.0525 - val_accuracy: 0.6001\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0603 - accuracy: 0.5121 - val_loss: 0.0507 - val_accuracy: 0.6224\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0587 - accuracy: 0.5296 - val_loss: 0.0485 - val_accuracy: 0.6440\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0571 - accuracy: 0.5501 - val_loss: 0.0464 - val_accuracy: 0.6640\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0551 - accuracy: 0.5708 - val_loss: 0.0440 - val_accuracy: 0.6838\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0534 - accuracy: 0.5877 - val_loss: 0.0416 - val_accuracy: 0.7040\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0515 - accuracy: 0.6092 - val_loss: 0.0394 - val_accuracy: 0.7215\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0499 - accuracy: 0.6240 - val_loss: 0.0374 - val_accuracy: 0.7424\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0483 - accuracy: 0.6410 - val_loss: 0.0355 - val_accuracy: 0.7523\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0469 - accuracy: 0.6528 - val_loss: 0.0339 - val_accuracy: 0.7675\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0454 - accuracy: 0.6668 - val_loss: 0.0322 - val_accuracy: 0.7830\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0439 - accuracy: 0.6805 - val_loss: 0.0309 - val_accuracy: 0.7930\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0424 - accuracy: 0.6920 - val_loss: 0.0294 - val_accuracy: 0.8043\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0413 - accuracy: 0.7048 - val_loss: 0.0281 - val_accuracy: 0.8145\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0399 - accuracy: 0.7161 - val_loss: 0.0269 - val_accuracy: 0.8214\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0388 - accuracy: 0.7247 - val_loss: 0.0258 - val_accuracy: 0.8303\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0376 - accuracy: 0.7365 - val_loss: 0.0247 - val_accuracy: 0.8376\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0367 - accuracy: 0.7444 - val_loss: 0.0239 - val_accuracy: 0.8415\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0355 - accuracy: 0.7541 - val_loss: 0.0229 - val_accuracy: 0.8497\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0346 - accuracy: 0.7618 - val_loss: 0.0223 - val_accuracy: 0.8533\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0334 - accuracy: 0.7696 - val_loss: 0.0212 - val_accuracy: 0.8605\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0323 - accuracy: 0.7812 - val_loss: 0.0205 - val_accuracy: 0.8644\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0314 - accuracy: 0.7862 - val_loss: 0.0197 - val_accuracy: 0.8704\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0308 - accuracy: 0.7912 - val_loss: 0.0192 - val_accuracy: 0.8733\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0297 - accuracy: 0.8001 - val_loss: 0.0187 - val_accuracy: 0.8776\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0293 - accuracy: 0.8039 - val_loss: 0.0183 - val_accuracy: 0.8785\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0286 - accuracy: 0.8078 - val_loss: 0.0177 - val_accuracy: 0.8833\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0279 - accuracy: 0.8125 - val_loss: 0.0173 - val_accuracy: 0.8865\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0274 - accuracy: 0.8176 - val_loss: 0.0170 - val_accuracy: 0.8878\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0266 - accuracy: 0.8231 - val_loss: 0.0165 - val_accuracy: 0.8918\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0260 - accuracy: 0.8277 - val_loss: 0.0161 - val_accuracy: 0.8939\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0254 - accuracy: 0.8327 - val_loss: 0.0158 - val_accuracy: 0.8963\n",
            "Accuracy for  100  epochs:\n",
            "Test loss: 0.015777649917826056\n",
            "Test accuracy: 0.8963000178337097 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TskdQAXHCDSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e7e5995-45b4-416d-e8f7-4472747fad1d"
      },
      "source": [
        "# Write your answer in here\n",
        "\n",
        "model = make_model(8, 256, 'relu')\n",
        "history = model.fit(x_train, y_train, batch_size = 64, epochs = 200, validation_data  =(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy for ', 200, ' epochs:')\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1], '\\n\\n')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"relu\", units=256)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0901 - accuracy: 0.1007 - val_loss: 0.0900 - val_accuracy: 0.1074\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0900 - accuracy: 0.1032 - val_loss: 0.0899 - val_accuracy: 0.1218\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0900 - accuracy: 0.1022 - val_loss: 0.0899 - val_accuracy: 0.1340\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0900 - accuracy: 0.1037 - val_loss: 0.0899 - val_accuracy: 0.1394\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0899 - accuracy: 0.1052 - val_loss: 0.0899 - val_accuracy: 0.1496\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1063 - val_loss: 0.0898 - val_accuracy: 0.1718\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0899 - accuracy: 0.1075 - val_loss: 0.0898 - val_accuracy: 0.1993\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0899 - accuracy: 0.1096 - val_loss: 0.0898 - val_accuracy: 0.2230\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0899 - accuracy: 0.1108 - val_loss: 0.0898 - val_accuracy: 0.2392\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0898 - accuracy: 0.1129 - val_loss: 0.0897 - val_accuracy: 0.2454\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0898 - accuracy: 0.1174 - val_loss: 0.0897 - val_accuracy: 0.2474\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0898 - accuracy: 0.1214 - val_loss: 0.0897 - val_accuracy: 0.2474\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1246 - val_loss: 0.0897 - val_accuracy: 0.2440\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0897 - accuracy: 0.1319 - val_loss: 0.0896 - val_accuracy: 0.2398\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1345 - val_loss: 0.0896 - val_accuracy: 0.2351\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0897 - accuracy: 0.1396 - val_loss: 0.0896 - val_accuracy: 0.2308\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0896 - accuracy: 0.1473 - val_loss: 0.0895 - val_accuracy: 0.2294\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0896 - accuracy: 0.1519 - val_loss: 0.0895 - val_accuracy: 0.2249\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0896 - accuracy: 0.1602 - val_loss: 0.0894 - val_accuracy: 0.2212\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0895 - accuracy: 0.1681 - val_loss: 0.0893 - val_accuracy: 0.2173\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0895 - accuracy: 0.1692 - val_loss: 0.0893 - val_accuracy: 0.2144\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0894 - accuracy: 0.1737 - val_loss: 0.0892 - val_accuracy: 0.2134\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0893 - accuracy: 0.1803 - val_loss: 0.0891 - val_accuracy: 0.2129\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0893 - accuracy: 0.1810 - val_loss: 0.0890 - val_accuracy: 0.2116\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0892 - accuracy: 0.1862 - val_loss: 0.0888 - val_accuracy: 0.2103\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0891 - accuracy: 0.1900 - val_loss: 0.0887 - val_accuracy: 0.2097\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0889 - accuracy: 0.1905 - val_loss: 0.0885 - val_accuracy: 0.2094\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0887 - accuracy: 0.1918 - val_loss: 0.0882 - val_accuracy: 0.2089\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0885 - accuracy: 0.1941 - val_loss: 0.0879 - val_accuracy: 0.2091\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0883 - accuracy: 0.1963 - val_loss: 0.0874 - val_accuracy: 0.2093\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0879 - accuracy: 0.1998 - val_loss: 0.0869 - val_accuracy: 0.2099\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0874 - accuracy: 0.2049 - val_loss: 0.0862 - val_accuracy: 0.2106\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0869 - accuracy: 0.2103 - val_loss: 0.0855 - val_accuracy: 0.2108\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0862 - accuracy: 0.2148 - val_loss: 0.0848 - val_accuracy: 0.2118\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0856 - accuracy: 0.2192 - val_loss: 0.0841 - val_accuracy: 0.2128\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0850 - accuracy: 0.2246 - val_loss: 0.0835 - val_accuracy: 0.2163\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0845 - accuracy: 0.2275 - val_loss: 0.0830 - val_accuracy: 0.2209\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0840 - accuracy: 0.2344 - val_loss: 0.0826 - val_accuracy: 0.2284\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0836 - accuracy: 0.2391 - val_loss: 0.0822 - val_accuracy: 0.2365\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0832 - accuracy: 0.2465 - val_loss: 0.0819 - val_accuracy: 0.2463\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0828 - accuracy: 0.2532 - val_loss: 0.0816 - val_accuracy: 0.2554\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0825 - accuracy: 0.2610 - val_loss: 0.0813 - val_accuracy: 0.2644\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0821 - accuracy: 0.2708 - val_loss: 0.0810 - val_accuracy: 0.2721\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0819 - accuracy: 0.2794 - val_loss: 0.0807 - val_accuracy: 0.2815\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0816 - accuracy: 0.2897 - val_loss: 0.0803 - val_accuracy: 0.2902\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0812 - accuracy: 0.3018 - val_loss: 0.0799 - val_accuracy: 0.3004\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0809 - accuracy: 0.3134 - val_loss: 0.0794 - val_accuracy: 0.3086\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0804 - accuracy: 0.3236 - val_loss: 0.0788 - val_accuracy: 0.3199\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0799 - accuracy: 0.3346 - val_loss: 0.0779 - val_accuracy: 0.3328\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0792 - accuracy: 0.3453 - val_loss: 0.0769 - val_accuracy: 0.3443\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0784 - accuracy: 0.3550 - val_loss: 0.0757 - val_accuracy: 0.3552\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0775 - accuracy: 0.3627 - val_loss: 0.0743 - val_accuracy: 0.3669\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0765 - accuracy: 0.3694 - val_loss: 0.0730 - val_accuracy: 0.3744\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0755 - accuracy: 0.3739 - val_loss: 0.0718 - val_accuracy: 0.3820\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0745 - accuracy: 0.3796 - val_loss: 0.0707 - val_accuracy: 0.3869\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0735 - accuracy: 0.3847 - val_loss: 0.0696 - val_accuracy: 0.3932\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0725 - accuracy: 0.3920 - val_loss: 0.0684 - val_accuracy: 0.4003\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0714 - accuracy: 0.4004 - val_loss: 0.0672 - val_accuracy: 0.4088\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0704 - accuracy: 0.4081 - val_loss: 0.0659 - val_accuracy: 0.4182\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0694 - accuracy: 0.4202 - val_loss: 0.0646 - val_accuracy: 0.4308\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0682 - accuracy: 0.4329 - val_loss: 0.0632 - val_accuracy: 0.4501\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0671 - accuracy: 0.4453 - val_loss: 0.0618 - val_accuracy: 0.4684\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0659 - accuracy: 0.4570 - val_loss: 0.0603 - val_accuracy: 0.4916\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0648 - accuracy: 0.4706 - val_loss: 0.0587 - val_accuracy: 0.5194\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0635 - accuracy: 0.4844 - val_loss: 0.0570 - val_accuracy: 0.5405\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0622 - accuracy: 0.4958 - val_loss: 0.0554 - val_accuracy: 0.5647\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0609 - accuracy: 0.5076 - val_loss: 0.0536 - val_accuracy: 0.5821\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0597 - accuracy: 0.5218 - val_loss: 0.0519 - val_accuracy: 0.5952\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0584 - accuracy: 0.5331 - val_loss: 0.0501 - val_accuracy: 0.6119\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0571 - accuracy: 0.5436 - val_loss: 0.0486 - val_accuracy: 0.6259\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0559 - accuracy: 0.5552 - val_loss: 0.0468 - val_accuracy: 0.6431\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0545 - accuracy: 0.5697 - val_loss: 0.0451 - val_accuracy: 0.6635\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0532 - accuracy: 0.5807 - val_loss: 0.0435 - val_accuracy: 0.6825\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0517 - accuracy: 0.5949 - val_loss: 0.0419 - val_accuracy: 0.7038\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0504 - accuracy: 0.6069 - val_loss: 0.0404 - val_accuracy: 0.7160\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0490 - accuracy: 0.6211 - val_loss: 0.0388 - val_accuracy: 0.7281\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0476 - accuracy: 0.6316 - val_loss: 0.0375 - val_accuracy: 0.7322\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0461 - accuracy: 0.6462 - val_loss: 0.0364 - val_accuracy: 0.7389\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0450 - accuracy: 0.6564 - val_loss: 0.0352 - val_accuracy: 0.7435\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0439 - accuracy: 0.6663 - val_loss: 0.0341 - val_accuracy: 0.7570\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0427 - accuracy: 0.6769 - val_loss: 0.0331 - val_accuracy: 0.7587\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0418 - accuracy: 0.6859 - val_loss: 0.0321 - val_accuracy: 0.7760\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0409 - accuracy: 0.6957 - val_loss: 0.0312 - val_accuracy: 0.7807\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0397 - accuracy: 0.7046 - val_loss: 0.0301 - val_accuracy: 0.7934\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0390 - accuracy: 0.7105 - val_loss: 0.0290 - val_accuracy: 0.8044\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0378 - accuracy: 0.7242 - val_loss: 0.0282 - val_accuracy: 0.8034\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0369 - accuracy: 0.7310 - val_loss: 0.0273 - val_accuracy: 0.8154\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0362 - accuracy: 0.7375 - val_loss: 0.0263 - val_accuracy: 0.8218\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0351 - accuracy: 0.7501 - val_loss: 0.0253 - val_accuracy: 0.8344\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0342 - accuracy: 0.7573 - val_loss: 0.0243 - val_accuracy: 0.8416\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0332 - accuracy: 0.7661 - val_loss: 0.0234 - val_accuracy: 0.8473\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0326 - accuracy: 0.7735 - val_loss: 0.0225 - val_accuracy: 0.8545\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0316 - accuracy: 0.7822 - val_loss: 0.0217 - val_accuracy: 0.8596\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0309 - accuracy: 0.7879 - val_loss: 0.0210 - val_accuracy: 0.8653\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0300 - accuracy: 0.7960 - val_loss: 0.0201 - val_accuracy: 0.8721\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0289 - accuracy: 0.8056 - val_loss: 0.0192 - val_accuracy: 0.8776\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0282 - accuracy: 0.8110 - val_loss: 0.0185 - val_accuracy: 0.8821\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0275 - accuracy: 0.8174 - val_loss: 0.0178 - val_accuracy: 0.8858\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0267 - accuracy: 0.8233 - val_loss: 0.0171 - val_accuracy: 0.8910\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0258 - accuracy: 0.8304 - val_loss: 0.0167 - val_accuracy: 0.8936\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0251 - accuracy: 0.8353 - val_loss: 0.0160 - val_accuracy: 0.8971\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0244 - accuracy: 0.8410 - val_loss: 0.0157 - val_accuracy: 0.9000\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0238 - accuracy: 0.8454 - val_loss: 0.0152 - val_accuracy: 0.9031\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0231 - accuracy: 0.8497 - val_loss: 0.0148 - val_accuracy: 0.9052\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0229 - accuracy: 0.8503 - val_loss: 0.0143 - val_accuracy: 0.9078\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0222 - accuracy: 0.8556 - val_loss: 0.0141 - val_accuracy: 0.9080\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0216 - accuracy: 0.8601 - val_loss: 0.0137 - val_accuracy: 0.9109\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0212 - accuracy: 0.8630 - val_loss: 0.0134 - val_accuracy: 0.9113\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0209 - accuracy: 0.8648 - val_loss: 0.0131 - val_accuracy: 0.9130\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0203 - accuracy: 0.8692 - val_loss: 0.0129 - val_accuracy: 0.9152\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0199 - accuracy: 0.8713 - val_loss: 0.0126 - val_accuracy: 0.9166\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0196 - accuracy: 0.8742 - val_loss: 0.0125 - val_accuracy: 0.9183\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0189 - accuracy: 0.8783 - val_loss: 0.0123 - val_accuracy: 0.9190\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0188 - accuracy: 0.8781 - val_loss: 0.0121 - val_accuracy: 0.9212\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0185 - accuracy: 0.8815 - val_loss: 0.0119 - val_accuracy: 0.9216\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0181 - accuracy: 0.8831 - val_loss: 0.0116 - val_accuracy: 0.9225\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0178 - accuracy: 0.8869 - val_loss: 0.0116 - val_accuracy: 0.9235\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0175 - accuracy: 0.8881 - val_loss: 0.0114 - val_accuracy: 0.9245\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0173 - accuracy: 0.8892 - val_loss: 0.0113 - val_accuracy: 0.9256\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0170 - accuracy: 0.8914 - val_loss: 0.0111 - val_accuracy: 0.9269\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0167 - accuracy: 0.8930 - val_loss: 0.0110 - val_accuracy: 0.9274\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0166 - accuracy: 0.8930 - val_loss: 0.0109 - val_accuracy: 0.9277\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0163 - accuracy: 0.8960 - val_loss: 0.0108 - val_accuracy: 0.9278\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - accuracy: 0.8960 - val_loss: 0.0106 - val_accuracy: 0.9298\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0159 - accuracy: 0.8985 - val_loss: 0.0105 - val_accuracy: 0.9308\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0156 - accuracy: 0.8990 - val_loss: 0.0104 - val_accuracy: 0.9313\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0156 - accuracy: 0.9003 - val_loss: 0.0103 - val_accuracy: 0.9329\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0154 - accuracy: 0.9015 - val_loss: 0.0102 - val_accuracy: 0.9335\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0151 - accuracy: 0.9029 - val_loss: 0.0101 - val_accuracy: 0.9343\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0151 - accuracy: 0.9032 - val_loss: 0.0100 - val_accuracy: 0.9353\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0147 - accuracy: 0.9068 - val_loss: 0.0099 - val_accuracy: 0.9360\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0146 - accuracy: 0.9062 - val_loss: 0.0098 - val_accuracy: 0.9366\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0145 - accuracy: 0.9086 - val_loss: 0.0098 - val_accuracy: 0.9373\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0143 - accuracy: 0.9085 - val_loss: 0.0097 - val_accuracy: 0.9374\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0142 - accuracy: 0.9096 - val_loss: 0.0096 - val_accuracy: 0.9381\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0141 - accuracy: 0.9101 - val_loss: 0.0096 - val_accuracy: 0.9389\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0139 - accuracy: 0.9111 - val_loss: 0.0094 - val_accuracy: 0.9395\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0137 - accuracy: 0.9123 - val_loss: 0.0094 - val_accuracy: 0.9394\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0136 - accuracy: 0.9122 - val_loss: 0.0093 - val_accuracy: 0.9406\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0135 - accuracy: 0.9137 - val_loss: 0.0093 - val_accuracy: 0.9398\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0135 - accuracy: 0.9135 - val_loss: 0.0092 - val_accuracy: 0.9408\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0134 - accuracy: 0.9148 - val_loss: 0.0091 - val_accuracy: 0.9424\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0131 - accuracy: 0.9164 - val_loss: 0.0091 - val_accuracy: 0.9419\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0130 - accuracy: 0.9165 - val_loss: 0.0090 - val_accuracy: 0.9429\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0130 - accuracy: 0.9167 - val_loss: 0.0090 - val_accuracy: 0.9431\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0128 - accuracy: 0.9183 - val_loss: 0.0089 - val_accuracy: 0.9433\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0128 - accuracy: 0.9187 - val_loss: 0.0088 - val_accuracy: 0.9447\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0127 - accuracy: 0.9189 - val_loss: 0.0088 - val_accuracy: 0.9447\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0125 - accuracy: 0.9199 - val_loss: 0.0087 - val_accuracy: 0.9451\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0124 - accuracy: 0.9204 - val_loss: 0.0087 - val_accuracy: 0.9453\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0124 - accuracy: 0.9207 - val_loss: 0.0086 - val_accuracy: 0.9458\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0121 - accuracy: 0.9238 - val_loss: 0.0086 - val_accuracy: 0.9456\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0121 - accuracy: 0.9229 - val_loss: 0.0085 - val_accuracy: 0.9464\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0119 - accuracy: 0.9242 - val_loss: 0.0085 - val_accuracy: 0.9468\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0119 - accuracy: 0.9238 - val_loss: 0.0084 - val_accuracy: 0.9469\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0118 - accuracy: 0.9251 - val_loss: 0.0083 - val_accuracy: 0.9472\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0117 - accuracy: 0.9255 - val_loss: 0.0083 - val_accuracy: 0.9469\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0116 - accuracy: 0.9261 - val_loss: 0.0083 - val_accuracy: 0.9477\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0114 - accuracy: 0.9276 - val_loss: 0.0083 - val_accuracy: 0.9477\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0114 - accuracy: 0.9270 - val_loss: 0.0082 - val_accuracy: 0.9486\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0114 - accuracy: 0.9269 - val_loss: 0.0082 - val_accuracy: 0.9489\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0113 - accuracy: 0.9276 - val_loss: 0.0081 - val_accuracy: 0.9480\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0112 - accuracy: 0.9292 - val_loss: 0.0081 - val_accuracy: 0.9493\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0110 - accuracy: 0.9299 - val_loss: 0.0081 - val_accuracy: 0.9486\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0110 - accuracy: 0.9299 - val_loss: 0.0080 - val_accuracy: 0.9495\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0110 - accuracy: 0.9294 - val_loss: 0.0080 - val_accuracy: 0.9485\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0110 - accuracy: 0.9296 - val_loss: 0.0079 - val_accuracy: 0.9494\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0110 - accuracy: 0.9297 - val_loss: 0.0079 - val_accuracy: 0.9494\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0109 - accuracy: 0.9309 - val_loss: 0.0078 - val_accuracy: 0.9498\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0108 - accuracy: 0.9316 - val_loss: 0.0078 - val_accuracy: 0.9505\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0106 - accuracy: 0.9321 - val_loss: 0.0077 - val_accuracy: 0.9504\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0107 - accuracy: 0.9319 - val_loss: 0.0077 - val_accuracy: 0.9504\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0104 - accuracy: 0.9338 - val_loss: 0.0076 - val_accuracy: 0.9512\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0105 - accuracy: 0.9334 - val_loss: 0.0076 - val_accuracy: 0.9504\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0105 - accuracy: 0.9331 - val_loss: 0.0076 - val_accuracy: 0.9511\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0103 - accuracy: 0.9345 - val_loss: 0.0076 - val_accuracy: 0.9517\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0102 - accuracy: 0.9351 - val_loss: 0.0076 - val_accuracy: 0.9512\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0102 - accuracy: 0.9357 - val_loss: 0.0075 - val_accuracy: 0.9523\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0102 - accuracy: 0.9352 - val_loss: 0.0074 - val_accuracy: 0.9530\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0102 - accuracy: 0.9352 - val_loss: 0.0074 - val_accuracy: 0.9528\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0100 - accuracy: 0.9366 - val_loss: 0.0073 - val_accuracy: 0.9533\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0098 - accuracy: 0.9378 - val_loss: 0.0073 - val_accuracy: 0.9534\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0098 - accuracy: 0.9375 - val_loss: 0.0073 - val_accuracy: 0.9531\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0099 - accuracy: 0.9369 - val_loss: 0.0072 - val_accuracy: 0.9535\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0098 - accuracy: 0.9369 - val_loss: 0.0072 - val_accuracy: 0.9539\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0098 - accuracy: 0.9383 - val_loss: 0.0072 - val_accuracy: 0.9551\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0096 - accuracy: 0.9393 - val_loss: 0.0071 - val_accuracy: 0.9546\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0097 - accuracy: 0.9384 - val_loss: 0.0071 - val_accuracy: 0.9552\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0096 - accuracy: 0.9392 - val_loss: 0.0071 - val_accuracy: 0.9548\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0095 - accuracy: 0.9395 - val_loss: 0.0071 - val_accuracy: 0.9548\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0094 - accuracy: 0.9400 - val_loss: 0.0070 - val_accuracy: 0.9557\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0093 - accuracy: 0.9412 - val_loss: 0.0070 - val_accuracy: 0.9555\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0094 - accuracy: 0.9402 - val_loss: 0.0070 - val_accuracy: 0.9556\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0093 - accuracy: 0.9413 - val_loss: 0.0070 - val_accuracy: 0.9556\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0092 - accuracy: 0.9416 - val_loss: 0.0069 - val_accuracy: 0.9566\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0092 - accuracy: 0.9416 - val_loss: 0.0069 - val_accuracy: 0.9571\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0093 - accuracy: 0.9407 - val_loss: 0.0069 - val_accuracy: 0.9571\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0091 - accuracy: 0.9424 - val_loss: 0.0068 - val_accuracy: 0.9574\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0091 - accuracy: 0.9422 - val_loss: 0.0068 - val_accuracy: 0.9570\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0091 - accuracy: 0.9420 - val_loss: 0.0068 - val_accuracy: 0.9570\n",
            "Accuracy for  200  epochs:\n",
            "Test loss: 0.006751826236258057\n",
            "Test accuracy: 0.9570000171661377 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFaT0oJmDKrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abd26670-8f94-4ced-c51e-47d540a57b60"
      },
      "source": [
        "# Write your answer in here\n",
        "\n",
        "model = make_model(8, 256, 'relu')\n",
        "history = model.fit(x_train, y_train, batch_size = 64, epochs = 300, validation_data  =(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Accuracy for ', 300, ' epochs:')\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1], '\\n\\n')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=784, activation=\"relu\", units=256)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/300\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0901 - accuracy: 0.1004 - val_loss: 0.0900 - val_accuracy: 0.1004\n",
            "Epoch 2/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0900 - accuracy: 0.1005 - val_loss: 0.0900 - val_accuracy: 0.1007\n",
            "Epoch 3/300\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0900 - accuracy: 0.1012 - val_loss: 0.0900 - val_accuracy: 0.1074\n",
            "Epoch 4/300\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0900 - accuracy: 0.1045 - val_loss: 0.0900 - val_accuracy: 0.1190\n",
            "Epoch 5/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0900 - accuracy: 0.1056 - val_loss: 0.0899 - val_accuracy: 0.1319\n",
            "Epoch 6/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0900 - accuracy: 0.1097 - val_loss: 0.0899 - val_accuracy: 0.1477\n",
            "Epoch 7/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0900 - accuracy: 0.1081 - val_loss: 0.0899 - val_accuracy: 0.1636\n",
            "Epoch 8/300\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0900 - accuracy: 0.1117 - val_loss: 0.0899 - val_accuracy: 0.1802\n",
            "Epoch 9/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0900 - accuracy: 0.1114 - val_loss: 0.0899 - val_accuracy: 0.1925\n",
            "Epoch 10/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1184 - val_loss: 0.0898 - val_accuracy: 0.2048\n",
            "Epoch 11/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1163 - val_loss: 0.0898 - val_accuracy: 0.2136\n",
            "Epoch 12/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1193 - val_loss: 0.0898 - val_accuracy: 0.2210\n",
            "Epoch 13/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1223 - val_loss: 0.0898 - val_accuracy: 0.2288\n",
            "Epoch 14/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1245 - val_loss: 0.0898 - val_accuracy: 0.2364\n",
            "Epoch 15/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0899 - accuracy: 0.1255 - val_loss: 0.0898 - val_accuracy: 0.2425\n",
            "Epoch 16/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1287 - val_loss: 0.0897 - val_accuracy: 0.2466\n",
            "Epoch 17/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0899 - accuracy: 0.1312 - val_loss: 0.0897 - val_accuracy: 0.2501\n",
            "Epoch 18/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0898 - accuracy: 0.1354 - val_loss: 0.0897 - val_accuracy: 0.2555\n",
            "Epoch 19/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1374 - val_loss: 0.0897 - val_accuracy: 0.2600\n",
            "Epoch 20/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0898 - accuracy: 0.1421 - val_loss: 0.0896 - val_accuracy: 0.2650\n",
            "Epoch 21/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0898 - accuracy: 0.1473 - val_loss: 0.0896 - val_accuracy: 0.2690\n",
            "Epoch 22/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0898 - accuracy: 0.1439 - val_loss: 0.0896 - val_accuracy: 0.2723\n",
            "Epoch 23/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0898 - accuracy: 0.1497 - val_loss: 0.0895 - val_accuracy: 0.2749\n",
            "Epoch 24/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1504 - val_loss: 0.0895 - val_accuracy: 0.2783\n",
            "Epoch 25/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1590 - val_loss: 0.0895 - val_accuracy: 0.2816\n",
            "Epoch 26/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0897 - accuracy: 0.1587 - val_loss: 0.0894 - val_accuracy: 0.2845\n",
            "Epoch 27/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0896 - accuracy: 0.1626 - val_loss: 0.0894 - val_accuracy: 0.2870\n",
            "Epoch 28/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0896 - accuracy: 0.1678 - val_loss: 0.0893 - val_accuracy: 0.2889\n",
            "Epoch 29/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0896 - accuracy: 0.1675 - val_loss: 0.0893 - val_accuracy: 0.2890\n",
            "Epoch 30/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0896 - accuracy: 0.1714 - val_loss: 0.0892 - val_accuracy: 0.2903\n",
            "Epoch 31/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0895 - accuracy: 0.1748 - val_loss: 0.0891 - val_accuracy: 0.2905\n",
            "Epoch 32/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0894 - accuracy: 0.1796 - val_loss: 0.0890 - val_accuracy: 0.2902\n",
            "Epoch 33/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0894 - accuracy: 0.1812 - val_loss: 0.0889 - val_accuracy: 0.2882\n",
            "Epoch 34/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0893 - accuracy: 0.1856 - val_loss: 0.0888 - val_accuracy: 0.2856\n",
            "Epoch 35/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0892 - accuracy: 0.1861 - val_loss: 0.0886 - val_accuracy: 0.2819\n",
            "Epoch 36/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0891 - accuracy: 0.1907 - val_loss: 0.0884 - val_accuracy: 0.2779\n",
            "Epoch 37/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0889 - accuracy: 0.1878 - val_loss: 0.0881 - val_accuracy: 0.2711\n",
            "Epoch 38/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0887 - accuracy: 0.1890 - val_loss: 0.0877 - val_accuracy: 0.2634\n",
            "Epoch 39/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0884 - accuracy: 0.1876 - val_loss: 0.0872 - val_accuracy: 0.2556\n",
            "Epoch 40/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0880 - accuracy: 0.1879 - val_loss: 0.0864 - val_accuracy: 0.2542\n",
            "Epoch 41/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0874 - accuracy: 0.1986 - val_loss: 0.0854 - val_accuracy: 0.2593\n",
            "Epoch 42/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0867 - accuracy: 0.2138 - val_loss: 0.0844 - val_accuracy: 0.2697\n",
            "Epoch 43/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0858 - accuracy: 0.2316 - val_loss: 0.0834 - val_accuracy: 0.2787\n",
            "Epoch 44/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0849 - accuracy: 0.2482 - val_loss: 0.0827 - val_accuracy: 0.2824\n",
            "Epoch 45/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0842 - accuracy: 0.2646 - val_loss: 0.0821 - val_accuracy: 0.2870\n",
            "Epoch 46/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0836 - accuracy: 0.2772 - val_loss: 0.0816 - val_accuracy: 0.2910\n",
            "Epoch 47/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0831 - accuracy: 0.2835 - val_loss: 0.0812 - val_accuracy: 0.2940\n",
            "Epoch 48/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0826 - accuracy: 0.2939 - val_loss: 0.0808 - val_accuracy: 0.2978\n",
            "Epoch 49/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0821 - accuracy: 0.3015 - val_loss: 0.0803 - val_accuracy: 0.3013\n",
            "Epoch 50/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0817 - accuracy: 0.3032 - val_loss: 0.0798 - val_accuracy: 0.3036\n",
            "Epoch 51/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0812 - accuracy: 0.3055 - val_loss: 0.0791 - val_accuracy: 0.3060\n",
            "Epoch 52/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0806 - accuracy: 0.3079 - val_loss: 0.0783 - val_accuracy: 0.3061\n",
            "Epoch 53/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0798 - accuracy: 0.3111 - val_loss: 0.0774 - val_accuracy: 0.3078\n",
            "Epoch 54/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0789 - accuracy: 0.3153 - val_loss: 0.0763 - val_accuracy: 0.3095\n",
            "Epoch 55/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0780 - accuracy: 0.3237 - val_loss: 0.0753 - val_accuracy: 0.3101\n",
            "Epoch 56/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0772 - accuracy: 0.3292 - val_loss: 0.0744 - val_accuracy: 0.3132\n",
            "Epoch 57/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0763 - accuracy: 0.3382 - val_loss: 0.0736 - val_accuracy: 0.3188\n",
            "Epoch 58/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0755 - accuracy: 0.3478 - val_loss: 0.0729 - val_accuracy: 0.3298\n",
            "Epoch 59/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0748 - accuracy: 0.3602 - val_loss: 0.0722 - val_accuracy: 0.3495\n",
            "Epoch 60/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0742 - accuracy: 0.3703 - val_loss: 0.0715 - val_accuracy: 0.3723\n",
            "Epoch 61/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0735 - accuracy: 0.3808 - val_loss: 0.0708 - val_accuracy: 0.3918\n",
            "Epoch 62/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0728 - accuracy: 0.3949 - val_loss: 0.0700 - val_accuracy: 0.4119\n",
            "Epoch 63/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0722 - accuracy: 0.4050 - val_loss: 0.0691 - val_accuracy: 0.4280\n",
            "Epoch 64/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0715 - accuracy: 0.4122 - val_loss: 0.0680 - val_accuracy: 0.4388\n",
            "Epoch 65/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0708 - accuracy: 0.4227 - val_loss: 0.0669 - val_accuracy: 0.4496\n",
            "Epoch 66/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0699 - accuracy: 0.4330 - val_loss: 0.0654 - val_accuracy: 0.4607\n",
            "Epoch 67/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0690 - accuracy: 0.4398 - val_loss: 0.0638 - val_accuracy: 0.4717\n",
            "Epoch 68/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0679 - accuracy: 0.4505 - val_loss: 0.0620 - val_accuracy: 0.4829\n",
            "Epoch 69/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0668 - accuracy: 0.4566 - val_loss: 0.0601 - val_accuracy: 0.5032\n",
            "Epoch 70/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0654 - accuracy: 0.4688 - val_loss: 0.0578 - val_accuracy: 0.5247\n",
            "Epoch 71/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0637 - accuracy: 0.4831 - val_loss: 0.0555 - val_accuracy: 0.5457\n",
            "Epoch 72/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0624 - accuracy: 0.4953 - val_loss: 0.0535 - val_accuracy: 0.5667\n",
            "Epoch 73/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0609 - accuracy: 0.5088 - val_loss: 0.0517 - val_accuracy: 0.5887\n",
            "Epoch 74/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0594 - accuracy: 0.5255 - val_loss: 0.0502 - val_accuracy: 0.6080\n",
            "Epoch 75/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0581 - accuracy: 0.5407 - val_loss: 0.0487 - val_accuracy: 0.6271\n",
            "Epoch 76/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0569 - accuracy: 0.5537 - val_loss: 0.0472 - val_accuracy: 0.6434\n",
            "Epoch 77/300\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0554 - accuracy: 0.5688 - val_loss: 0.0459 - val_accuracy: 0.6592\n",
            "Epoch 78/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0543 - accuracy: 0.5810 - val_loss: 0.0445 - val_accuracy: 0.6751\n",
            "Epoch 79/300\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0529 - accuracy: 0.5933 - val_loss: 0.0429 - val_accuracy: 0.6901\n",
            "Epoch 80/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0516 - accuracy: 0.6058 - val_loss: 0.0413 - val_accuracy: 0.7076\n",
            "Epoch 81/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0504 - accuracy: 0.6163 - val_loss: 0.0398 - val_accuracy: 0.7251\n",
            "Epoch 82/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0490 - accuracy: 0.6326 - val_loss: 0.0381 - val_accuracy: 0.7365\n",
            "Epoch 83/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0477 - accuracy: 0.6449 - val_loss: 0.0365 - val_accuracy: 0.7544\n",
            "Epoch 84/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0463 - accuracy: 0.6590 - val_loss: 0.0349 - val_accuracy: 0.7699\n",
            "Epoch 85/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0449 - accuracy: 0.6716 - val_loss: 0.0330 - val_accuracy: 0.7947\n",
            "Epoch 86/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0435 - accuracy: 0.6861 - val_loss: 0.0312 - val_accuracy: 0.8077\n",
            "Epoch 87/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0417 - accuracy: 0.7010 - val_loss: 0.0294 - val_accuracy: 0.8191\n",
            "Epoch 88/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0403 - accuracy: 0.7171 - val_loss: 0.0273 - val_accuracy: 0.8335\n",
            "Epoch 89/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0387 - accuracy: 0.7303 - val_loss: 0.0257 - val_accuracy: 0.8414\n",
            "Epoch 90/300\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0370 - accuracy: 0.7434 - val_loss: 0.0241 - val_accuracy: 0.8497\n",
            "Epoch 91/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0355 - accuracy: 0.7559 - val_loss: 0.0225 - val_accuracy: 0.8593\n",
            "Epoch 92/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0342 - accuracy: 0.7670 - val_loss: 0.0212 - val_accuracy: 0.8664\n",
            "Epoch 93/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0327 - accuracy: 0.7786 - val_loss: 0.0201 - val_accuracy: 0.8727\n",
            "Epoch 94/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0316 - accuracy: 0.7863 - val_loss: 0.0191 - val_accuracy: 0.8776\n",
            "Epoch 95/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0302 - accuracy: 0.7963 - val_loss: 0.0184 - val_accuracy: 0.8803\n",
            "Epoch 96/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0291 - accuracy: 0.8052 - val_loss: 0.0179 - val_accuracy: 0.8838\n",
            "Epoch 97/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0282 - accuracy: 0.8123 - val_loss: 0.0173 - val_accuracy: 0.8866\n",
            "Epoch 98/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0272 - accuracy: 0.8186 - val_loss: 0.0168 - val_accuracy: 0.8890\n",
            "Epoch 99/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0264 - accuracy: 0.8241 - val_loss: 0.0163 - val_accuracy: 0.8915\n",
            "Epoch 100/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0258 - accuracy: 0.8294 - val_loss: 0.0159 - val_accuracy: 0.8938\n",
            "Epoch 101/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0253 - accuracy: 0.8323 - val_loss: 0.0156 - val_accuracy: 0.8966\n",
            "Epoch 102/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0246 - accuracy: 0.8368 - val_loss: 0.0152 - val_accuracy: 0.8994\n",
            "Epoch 103/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0238 - accuracy: 0.8425 - val_loss: 0.0150 - val_accuracy: 0.8999\n",
            "Epoch 104/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0232 - accuracy: 0.8481 - val_loss: 0.0148 - val_accuracy: 0.9022\n",
            "Epoch 105/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0229 - accuracy: 0.8492 - val_loss: 0.0145 - val_accuracy: 0.9041\n",
            "Epoch 106/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0226 - accuracy: 0.8524 - val_loss: 0.0142 - val_accuracy: 0.9057\n",
            "Epoch 107/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0220 - accuracy: 0.8551 - val_loss: 0.0141 - val_accuracy: 0.9066\n",
            "Epoch 108/300\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0215 - accuracy: 0.8591 - val_loss: 0.0138 - val_accuracy: 0.9081\n",
            "Epoch 109/300\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0211 - accuracy: 0.8619 - val_loss: 0.0136 - val_accuracy: 0.9108\n",
            "Epoch 110/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0207 - accuracy: 0.8645 - val_loss: 0.0134 - val_accuracy: 0.9118\n",
            "Epoch 111/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0205 - accuracy: 0.8666 - val_loss: 0.0132 - val_accuracy: 0.9133\n",
            "Epoch 112/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0201 - accuracy: 0.8691 - val_loss: 0.0131 - val_accuracy: 0.9142\n",
            "Epoch 113/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0196 - accuracy: 0.8724 - val_loss: 0.0129 - val_accuracy: 0.9148\n",
            "Epoch 114/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0194 - accuracy: 0.8739 - val_loss: 0.0127 - val_accuracy: 0.9156\n",
            "Epoch 115/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0190 - accuracy: 0.8765 - val_loss: 0.0125 - val_accuracy: 0.9174\n",
            "Epoch 116/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0188 - accuracy: 0.8774 - val_loss: 0.0124 - val_accuracy: 0.9183\n",
            "Epoch 117/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0185 - accuracy: 0.8794 - val_loss: 0.0122 - val_accuracy: 0.9194\n",
            "Epoch 118/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0182 - accuracy: 0.8823 - val_loss: 0.0121 - val_accuracy: 0.9200\n",
            "Epoch 119/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0179 - accuracy: 0.8847 - val_loss: 0.0119 - val_accuracy: 0.9213\n",
            "Epoch 120/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0177 - accuracy: 0.8847 - val_loss: 0.0118 - val_accuracy: 0.9228\n",
            "Epoch 121/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0176 - accuracy: 0.8861 - val_loss: 0.0117 - val_accuracy: 0.9233\n",
            "Epoch 122/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0172 - accuracy: 0.8903 - val_loss: 0.0115 - val_accuracy: 0.9251\n",
            "Epoch 123/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0170 - accuracy: 0.8898 - val_loss: 0.0114 - val_accuracy: 0.9256\n",
            "Epoch 124/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0168 - accuracy: 0.8915 - val_loss: 0.0113 - val_accuracy: 0.9261\n",
            "Epoch 125/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0165 - accuracy: 0.8939 - val_loss: 0.0112 - val_accuracy: 0.9269\n",
            "Epoch 126/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0164 - accuracy: 0.8937 - val_loss: 0.0110 - val_accuracy: 0.9277\n",
            "Epoch 127/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0162 - accuracy: 0.8960 - val_loss: 0.0109 - val_accuracy: 0.9285\n",
            "Epoch 128/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0161 - accuracy: 0.8961 - val_loss: 0.0108 - val_accuracy: 0.9295\n",
            "Epoch 129/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0158 - accuracy: 0.8978 - val_loss: 0.0107 - val_accuracy: 0.9293\n",
            "Epoch 130/300\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0156 - accuracy: 0.9001 - val_loss: 0.0106 - val_accuracy: 0.9310\n",
            "Epoch 131/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0153 - accuracy: 0.9019 - val_loss: 0.0105 - val_accuracy: 0.9311\n",
            "Epoch 132/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0153 - accuracy: 0.9022 - val_loss: 0.0105 - val_accuracy: 0.9311\n",
            "Epoch 133/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0152 - accuracy: 0.9023 - val_loss: 0.0103 - val_accuracy: 0.9336\n",
            "Epoch 134/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0149 - accuracy: 0.9046 - val_loss: 0.0102 - val_accuracy: 0.9340\n",
            "Epoch 135/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0147 - accuracy: 0.9059 - val_loss: 0.0101 - val_accuracy: 0.9353\n",
            "Epoch 136/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0147 - accuracy: 0.9057 - val_loss: 0.0100 - val_accuracy: 0.9356\n",
            "Epoch 137/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0144 - accuracy: 0.9071 - val_loss: 0.0099 - val_accuracy: 0.9365\n",
            "Epoch 138/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0143 - accuracy: 0.9090 - val_loss: 0.0099 - val_accuracy: 0.9361\n",
            "Epoch 139/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0142 - accuracy: 0.9089 - val_loss: 0.0098 - val_accuracy: 0.9369\n",
            "Epoch 140/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0141 - accuracy: 0.9095 - val_loss: 0.0097 - val_accuracy: 0.9378\n",
            "Epoch 141/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0139 - accuracy: 0.9110 - val_loss: 0.0096 - val_accuracy: 0.9391\n",
            "Epoch 142/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0137 - accuracy: 0.9120 - val_loss: 0.0095 - val_accuracy: 0.9392\n",
            "Epoch 143/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0136 - accuracy: 0.9133 - val_loss: 0.0095 - val_accuracy: 0.9401\n",
            "Epoch 144/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0135 - accuracy: 0.9133 - val_loss: 0.0093 - val_accuracy: 0.9406\n",
            "Epoch 145/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0132 - accuracy: 0.9153 - val_loss: 0.0093 - val_accuracy: 0.9403\n",
            "Epoch 146/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0131 - accuracy: 0.9159 - val_loss: 0.0093 - val_accuracy: 0.9403\n",
            "Epoch 147/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0131 - accuracy: 0.9160 - val_loss: 0.0092 - val_accuracy: 0.9412\n",
            "Epoch 148/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0131 - accuracy: 0.9158 - val_loss: 0.0091 - val_accuracy: 0.9410\n",
            "Epoch 149/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0128 - accuracy: 0.9183 - val_loss: 0.0090 - val_accuracy: 0.9425\n",
            "Epoch 150/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0129 - accuracy: 0.9174 - val_loss: 0.0090 - val_accuracy: 0.9429\n",
            "Epoch 151/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0126 - accuracy: 0.9193 - val_loss: 0.0089 - val_accuracy: 0.9437\n",
            "Epoch 152/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0125 - accuracy: 0.9199 - val_loss: 0.0088 - val_accuracy: 0.9436\n",
            "Epoch 153/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0125 - accuracy: 0.9195 - val_loss: 0.0088 - val_accuracy: 0.9434\n",
            "Epoch 154/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0125 - accuracy: 0.9204 - val_loss: 0.0087 - val_accuracy: 0.9437\n",
            "Epoch 155/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0122 - accuracy: 0.9217 - val_loss: 0.0086 - val_accuracy: 0.9448\n",
            "Epoch 156/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0121 - accuracy: 0.9230 - val_loss: 0.0086 - val_accuracy: 0.9447\n",
            "Epoch 157/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0121 - accuracy: 0.9228 - val_loss: 0.0085 - val_accuracy: 0.9453\n",
            "Epoch 158/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0120 - accuracy: 0.9231 - val_loss: 0.0085 - val_accuracy: 0.9455\n",
            "Epoch 159/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0119 - accuracy: 0.9239 - val_loss: 0.0084 - val_accuracy: 0.9459\n",
            "Epoch 160/300\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0119 - accuracy: 0.9241 - val_loss: 0.0084 - val_accuracy: 0.9464\n",
            "Epoch 161/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0117 - accuracy: 0.9253 - val_loss: 0.0083 - val_accuracy: 0.9461\n",
            "Epoch 162/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0117 - accuracy: 0.9262 - val_loss: 0.0082 - val_accuracy: 0.9472\n",
            "Epoch 163/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0117 - accuracy: 0.9251 - val_loss: 0.0082 - val_accuracy: 0.9476\n",
            "Epoch 164/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0116 - accuracy: 0.9259 - val_loss: 0.0082 - val_accuracy: 0.9477\n",
            "Epoch 165/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0115 - accuracy: 0.9272 - val_loss: 0.0081 - val_accuracy: 0.9479\n",
            "Epoch 166/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0114 - accuracy: 0.9274 - val_loss: 0.0081 - val_accuracy: 0.9483\n",
            "Epoch 167/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0112 - accuracy: 0.9284 - val_loss: 0.0080 - val_accuracy: 0.9492\n",
            "Epoch 168/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0112 - accuracy: 0.9286 - val_loss: 0.0080 - val_accuracy: 0.9491\n",
            "Epoch 169/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0111 - accuracy: 0.9291 - val_loss: 0.0079 - val_accuracy: 0.9499\n",
            "Epoch 170/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0110 - accuracy: 0.9298 - val_loss: 0.0079 - val_accuracy: 0.9497\n",
            "Epoch 171/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0109 - accuracy: 0.9300 - val_loss: 0.0078 - val_accuracy: 0.9500\n",
            "Epoch 172/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0107 - accuracy: 0.9316 - val_loss: 0.0078 - val_accuracy: 0.9512\n",
            "Epoch 173/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0108 - accuracy: 0.9303 - val_loss: 0.0078 - val_accuracy: 0.9502\n",
            "Epoch 174/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0107 - accuracy: 0.9316 - val_loss: 0.0078 - val_accuracy: 0.9510\n",
            "Epoch 175/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0107 - accuracy: 0.9314 - val_loss: 0.0077 - val_accuracy: 0.9513\n",
            "Epoch 176/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0105 - accuracy: 0.9331 - val_loss: 0.0077 - val_accuracy: 0.9515\n",
            "Epoch 177/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0105 - accuracy: 0.9338 - val_loss: 0.0076 - val_accuracy: 0.9519\n",
            "Epoch 178/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0106 - accuracy: 0.9338 - val_loss: 0.0076 - val_accuracy: 0.9519\n",
            "Epoch 179/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0104 - accuracy: 0.9341 - val_loss: 0.0075 - val_accuracy: 0.9528\n",
            "Epoch 180/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0103 - accuracy: 0.9345 - val_loss: 0.0075 - val_accuracy: 0.9524\n",
            "Epoch 181/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0102 - accuracy: 0.9356 - val_loss: 0.0075 - val_accuracy: 0.9526\n",
            "Epoch 182/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0102 - accuracy: 0.9353 - val_loss: 0.0075 - val_accuracy: 0.9530\n",
            "Epoch 183/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0101 - accuracy: 0.9355 - val_loss: 0.0074 - val_accuracy: 0.9520\n",
            "Epoch 184/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0102 - accuracy: 0.9347 - val_loss: 0.0074 - val_accuracy: 0.9537\n",
            "Epoch 185/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0099 - accuracy: 0.9371 - val_loss: 0.0074 - val_accuracy: 0.9530\n",
            "Epoch 186/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0100 - accuracy: 0.9368 - val_loss: 0.0073 - val_accuracy: 0.9533\n",
            "Epoch 187/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0098 - accuracy: 0.9375 - val_loss: 0.0072 - val_accuracy: 0.9532\n",
            "Epoch 188/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0099 - accuracy: 0.9373 - val_loss: 0.0072 - val_accuracy: 0.9533\n",
            "Epoch 189/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0099 - accuracy: 0.9371 - val_loss: 0.0072 - val_accuracy: 0.9536\n",
            "Epoch 190/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0097 - accuracy: 0.9383 - val_loss: 0.0072 - val_accuracy: 0.9538\n",
            "Epoch 191/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0098 - accuracy: 0.9385 - val_loss: 0.0071 - val_accuracy: 0.9541\n",
            "Epoch 192/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0096 - accuracy: 0.9392 - val_loss: 0.0071 - val_accuracy: 0.9541\n",
            "Epoch 193/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0096 - accuracy: 0.9391 - val_loss: 0.0071 - val_accuracy: 0.9548\n",
            "Epoch 194/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0095 - accuracy: 0.9410 - val_loss: 0.0071 - val_accuracy: 0.9545\n",
            "Epoch 195/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0094 - accuracy: 0.9406 - val_loss: 0.0070 - val_accuracy: 0.9547\n",
            "Epoch 196/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0094 - accuracy: 0.9402 - val_loss: 0.0070 - val_accuracy: 0.9549\n",
            "Epoch 197/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0093 - accuracy: 0.9408 - val_loss: 0.0070 - val_accuracy: 0.9558\n",
            "Epoch 198/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0092 - accuracy: 0.9418 - val_loss: 0.0069 - val_accuracy: 0.9560\n",
            "Epoch 199/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0092 - accuracy: 0.9419 - val_loss: 0.0069 - val_accuracy: 0.9563\n",
            "Epoch 200/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0094 - accuracy: 0.9401 - val_loss: 0.0069 - val_accuracy: 0.9561\n",
            "Epoch 201/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0091 - accuracy: 0.9423 - val_loss: 0.0068 - val_accuracy: 0.9571\n",
            "Epoch 202/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0090 - accuracy: 0.9427 - val_loss: 0.0068 - val_accuracy: 0.9569\n",
            "Epoch 203/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0090 - accuracy: 0.9429 - val_loss: 0.0068 - val_accuracy: 0.9571\n",
            "Epoch 204/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0090 - accuracy: 0.9429 - val_loss: 0.0068 - val_accuracy: 0.9566\n",
            "Epoch 205/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0090 - accuracy: 0.9429 - val_loss: 0.0067 - val_accuracy: 0.9567\n",
            "Epoch 206/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0089 - accuracy: 0.9428 - val_loss: 0.0068 - val_accuracy: 0.9569\n",
            "Epoch 207/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0089 - accuracy: 0.9436 - val_loss: 0.0067 - val_accuracy: 0.9575\n",
            "Epoch 208/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0089 - accuracy: 0.9431 - val_loss: 0.0067 - val_accuracy: 0.9575\n",
            "Epoch 209/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0088 - accuracy: 0.9448 - val_loss: 0.0066 - val_accuracy: 0.9581\n",
            "Epoch 210/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0088 - accuracy: 0.9445 - val_loss: 0.0066 - val_accuracy: 0.9577\n",
            "Epoch 211/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0088 - accuracy: 0.9446 - val_loss: 0.0066 - val_accuracy: 0.9575\n",
            "Epoch 212/300\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0087 - accuracy: 0.9448 - val_loss: 0.0065 - val_accuracy: 0.9580\n",
            "Epoch 213/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0086 - accuracy: 0.9453 - val_loss: 0.0066 - val_accuracy: 0.9583\n",
            "Epoch 214/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0085 - accuracy: 0.9458 - val_loss: 0.0065 - val_accuracy: 0.9586\n",
            "Epoch 215/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0084 - accuracy: 0.9466 - val_loss: 0.0065 - val_accuracy: 0.9587\n",
            "Epoch 216/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0086 - accuracy: 0.9455 - val_loss: 0.0065 - val_accuracy: 0.9587\n",
            "Epoch 217/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0085 - accuracy: 0.9462 - val_loss: 0.0065 - val_accuracy: 0.9588\n",
            "Epoch 218/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0083 - accuracy: 0.9472 - val_loss: 0.0064 - val_accuracy: 0.9592\n",
            "Epoch 219/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0084 - accuracy: 0.9467 - val_loss: 0.0064 - val_accuracy: 0.9585\n",
            "Epoch 220/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0085 - accuracy: 0.9462 - val_loss: 0.0064 - val_accuracy: 0.9597\n",
            "Epoch 221/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0083 - accuracy: 0.9477 - val_loss: 0.0064 - val_accuracy: 0.9593\n",
            "Epoch 222/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0083 - accuracy: 0.9471 - val_loss: 0.0064 - val_accuracy: 0.9588\n",
            "Epoch 223/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0083 - accuracy: 0.9471 - val_loss: 0.0063 - val_accuracy: 0.9596\n",
            "Epoch 224/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0082 - accuracy: 0.9482 - val_loss: 0.0063 - val_accuracy: 0.9594\n",
            "Epoch 225/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0083 - accuracy: 0.9478 - val_loss: 0.0063 - val_accuracy: 0.9597\n",
            "Epoch 226/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0081 - accuracy: 0.9489 - val_loss: 0.0063 - val_accuracy: 0.9598\n",
            "Epoch 227/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0082 - accuracy: 0.9485 - val_loss: 0.0063 - val_accuracy: 0.9601\n",
            "Epoch 228/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0081 - accuracy: 0.9496 - val_loss: 0.0062 - val_accuracy: 0.9608\n",
            "Epoch 229/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0080 - accuracy: 0.9492 - val_loss: 0.0062 - val_accuracy: 0.9607\n",
            "Epoch 230/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0080 - accuracy: 0.9496 - val_loss: 0.0062 - val_accuracy: 0.9606\n",
            "Epoch 231/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0079 - accuracy: 0.9499 - val_loss: 0.0062 - val_accuracy: 0.9613\n",
            "Epoch 232/300\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0081 - accuracy: 0.9485 - val_loss: 0.0062 - val_accuracy: 0.9599\n",
            "Epoch 233/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0079 - accuracy: 0.9503 - val_loss: 0.0061 - val_accuracy: 0.9614\n",
            "Epoch 234/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0079 - accuracy: 0.9498 - val_loss: 0.0061 - val_accuracy: 0.9603\n",
            "Epoch 235/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0079 - accuracy: 0.9505 - val_loss: 0.0061 - val_accuracy: 0.9614\n",
            "Epoch 236/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0078 - accuracy: 0.9509 - val_loss: 0.0061 - val_accuracy: 0.9616\n",
            "Epoch 237/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0077 - accuracy: 0.9515 - val_loss: 0.0061 - val_accuracy: 0.9617\n",
            "Epoch 238/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0077 - accuracy: 0.9516 - val_loss: 0.0061 - val_accuracy: 0.9617\n",
            "Epoch 239/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0076 - accuracy: 0.9523 - val_loss: 0.0060 - val_accuracy: 0.9616\n",
            "Epoch 240/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0076 - accuracy: 0.9514 - val_loss: 0.0060 - val_accuracy: 0.9619\n",
            "Epoch 241/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0076 - accuracy: 0.9516 - val_loss: 0.0060 - val_accuracy: 0.9617\n",
            "Epoch 242/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0076 - accuracy: 0.9517 - val_loss: 0.0060 - val_accuracy: 0.9621\n",
            "Epoch 243/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0074 - accuracy: 0.9531 - val_loss: 0.0060 - val_accuracy: 0.9620\n",
            "Epoch 244/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0074 - accuracy: 0.9534 - val_loss: 0.0059 - val_accuracy: 0.9626\n",
            "Epoch 245/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0075 - accuracy: 0.9528 - val_loss: 0.0059 - val_accuracy: 0.9623\n",
            "Epoch 246/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0075 - accuracy: 0.9528 - val_loss: 0.0059 - val_accuracy: 0.9624\n",
            "Epoch 247/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0074 - accuracy: 0.9534 - val_loss: 0.0059 - val_accuracy: 0.9628\n",
            "Epoch 248/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0074 - accuracy: 0.9521 - val_loss: 0.0059 - val_accuracy: 0.9633\n",
            "Epoch 249/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0073 - accuracy: 0.9543 - val_loss: 0.0059 - val_accuracy: 0.9633\n",
            "Epoch 250/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0074 - accuracy: 0.9527 - val_loss: 0.0059 - val_accuracy: 0.9629\n",
            "Epoch 251/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0073 - accuracy: 0.9533 - val_loss: 0.0058 - val_accuracy: 0.9626\n",
            "Epoch 252/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0073 - accuracy: 0.9539 - val_loss: 0.0058 - val_accuracy: 0.9635\n",
            "Epoch 253/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0072 - accuracy: 0.9548 - val_loss: 0.0058 - val_accuracy: 0.9636\n",
            "Epoch 254/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0072 - accuracy: 0.9551 - val_loss: 0.0058 - val_accuracy: 0.9631\n",
            "Epoch 255/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0071 - accuracy: 0.9548 - val_loss: 0.0058 - val_accuracy: 0.9639\n",
            "Epoch 256/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0072 - accuracy: 0.9546 - val_loss: 0.0058 - val_accuracy: 0.9636\n",
            "Epoch 257/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0072 - accuracy: 0.9549 - val_loss: 0.0058 - val_accuracy: 0.9634\n",
            "Epoch 258/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0071 - accuracy: 0.9559 - val_loss: 0.0057 - val_accuracy: 0.9636\n",
            "Epoch 259/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0070 - accuracy: 0.9551 - val_loss: 0.0058 - val_accuracy: 0.9635\n",
            "Epoch 260/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0071 - accuracy: 0.9555 - val_loss: 0.0057 - val_accuracy: 0.9639\n",
            "Epoch 261/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - accuracy: 0.9560 - val_loss: 0.0057 - val_accuracy: 0.9643\n",
            "Epoch 262/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0070 - accuracy: 0.9557 - val_loss: 0.0057 - val_accuracy: 0.9642\n",
            "Epoch 263/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0071 - accuracy: 0.9554 - val_loss: 0.0057 - val_accuracy: 0.9642\n",
            "Epoch 264/300\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.0069 - accuracy: 0.9566 - val_loss: 0.0057 - val_accuracy: 0.9639\n",
            "Epoch 265/300\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0070 - accuracy: 0.9552 - val_loss: 0.0057 - val_accuracy: 0.9645\n",
            "Epoch 266/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - accuracy: 0.9566 - val_loss: 0.0057 - val_accuracy: 0.9642\n",
            "Epoch 267/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - accuracy: 0.9564 - val_loss: 0.0056 - val_accuracy: 0.9647\n",
            "Epoch 268/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - accuracy: 0.9573 - val_loss: 0.0056 - val_accuracy: 0.9650\n",
            "Epoch 269/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0069 - accuracy: 0.9568 - val_loss: 0.0056 - val_accuracy: 0.9652\n",
            "Epoch 270/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0068 - accuracy: 0.9571 - val_loss: 0.0056 - val_accuracy: 0.9653\n",
            "Epoch 271/300\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0069 - accuracy: 0.9570 - val_loss: 0.0056 - val_accuracy: 0.9656\n",
            "Epoch 272/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0067 - accuracy: 0.9582 - val_loss: 0.0055 - val_accuracy: 0.9656\n",
            "Epoch 273/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0067 - accuracy: 0.9579 - val_loss: 0.0055 - val_accuracy: 0.9649\n",
            "Epoch 274/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0067 - accuracy: 0.9582 - val_loss: 0.0055 - val_accuracy: 0.9657\n",
            "Epoch 275/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0067 - accuracy: 0.9572 - val_loss: 0.0055 - val_accuracy: 0.9659\n",
            "Epoch 276/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0066 - accuracy: 0.9581 - val_loss: 0.0055 - val_accuracy: 0.9661\n",
            "Epoch 277/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0065 - accuracy: 0.9585 - val_loss: 0.0055 - val_accuracy: 0.9661\n",
            "Epoch 278/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0066 - accuracy: 0.9587 - val_loss: 0.0055 - val_accuracy: 0.9662\n",
            "Epoch 279/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0066 - accuracy: 0.9586 - val_loss: 0.0055 - val_accuracy: 0.9655\n",
            "Epoch 280/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0066 - accuracy: 0.9590 - val_loss: 0.0055 - val_accuracy: 0.9656\n",
            "Epoch 281/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0065 - accuracy: 0.9589 - val_loss: 0.0054 - val_accuracy: 0.9657\n",
            "Epoch 282/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0066 - accuracy: 0.9589 - val_loss: 0.0055 - val_accuracy: 0.9656\n",
            "Epoch 283/300\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.0065 - accuracy: 0.9593 - val_loss: 0.0054 - val_accuracy: 0.9662\n",
            "Epoch 284/300\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0065 - accuracy: 0.9593 - val_loss: 0.0054 - val_accuracy: 0.9658\n",
            "Epoch 285/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0064 - accuracy: 0.9597 - val_loss: 0.0054 - val_accuracy: 0.9665\n",
            "Epoch 286/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0064 - accuracy: 0.9593 - val_loss: 0.0054 - val_accuracy: 0.9664\n",
            "Epoch 287/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0063 - accuracy: 0.9602 - val_loss: 0.0054 - val_accuracy: 0.9662\n",
            "Epoch 288/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0064 - accuracy: 0.9595 - val_loss: 0.0053 - val_accuracy: 0.9665\n",
            "Epoch 289/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0063 - accuracy: 0.9602 - val_loss: 0.0053 - val_accuracy: 0.9662\n",
            "Epoch 290/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0063 - accuracy: 0.9607 - val_loss: 0.0054 - val_accuracy: 0.9669\n",
            "Epoch 291/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0063 - accuracy: 0.9600 - val_loss: 0.0053 - val_accuracy: 0.9666\n",
            "Epoch 292/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0063 - accuracy: 0.9608 - val_loss: 0.0053 - val_accuracy: 0.9667\n",
            "Epoch 293/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0062 - accuracy: 0.9608 - val_loss: 0.0053 - val_accuracy: 0.9666\n",
            "Epoch 294/300\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0062 - accuracy: 0.9611 - val_loss: 0.0053 - val_accuracy: 0.9665\n",
            "Epoch 295/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0062 - accuracy: 0.9610 - val_loss: 0.0053 - val_accuracy: 0.9669\n",
            "Epoch 296/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0063 - accuracy: 0.9602 - val_loss: 0.0053 - val_accuracy: 0.9674\n",
            "Epoch 297/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0062 - accuracy: 0.9612 - val_loss: 0.0052 - val_accuracy: 0.9670\n",
            "Epoch 298/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0063 - accuracy: 0.9610 - val_loss: 0.0052 - val_accuracy: 0.9671\n",
            "Epoch 299/300\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0062 - accuracy: 0.9610 - val_loss: 0.0052 - val_accuracy: 0.9673\n",
            "Epoch 300/300\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0061 - accuracy: 0.9616 - val_loss: 0.0052 - val_accuracy: 0.9675\n",
            "Accuracy for  300  epochs:\n",
            "Test loss: 0.005198546393083245\n",
            "Test accuracy: 0.9674999713897705 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rda2RcCCBkG0",
        "colab_type": "text"
      },
      "source": [
        "## 5. Plot the test loss & test accuracy about the above result(Problem 4: Epochs- 300) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lNpTu6NBjDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "3db7df3b-d17e-42bf-954c-2fac5bd14de7"
      },
      "source": [
        "# Write your answer in here\n",
        "plt.plot(history.history['val_loss'], label='Test Loss')\n",
        "plt.plot(history.history['val_accuracy'], label = 'Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f759698b0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFPkxxD9dze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}